{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The global warming issue and Narratives around it<br>\n",
    "### Part 6: Testing the trained model onto a new dataset\n",
    "\n",
    "In this notebook, I tested the trained NLP model onto a new dataset, which belonged to a more general class of [climate change](https://www.reddit.com/r/climatechange/) issues. The hope was to filter the global warming concerns from the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "#Importing the built function, prior to that added the assets path to the system path\n",
    "#Inspiration: https://stackoverflow.com/questions/4383571/importing-files-from-different-folder\n",
    "\n",
    "import sys\n",
    "# inserting the parent directory into current path\n",
    "sys.path.insert(1, '../assets')\n",
    "\n",
    "from get_reddit_posts import get_reddit_posts\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from nltk.corpus import stopwords # Import the stopword list\n",
    "import pickle\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import regex as re\n",
    "import nltk\n",
    "#nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling in the created function and reading global warming data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 posts downloaded, oldest post:2020-06-20 10:40:38 - status code: 200, now waiting 1 seconds before next pull. Patience...\n",
      "\u001b[92m Does the imported dataframe match the request? True\u001b[00m\n",
      "Final DataFrame shape: (100, 71), there are 0 duplicates\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Defining API pull initial parameteres:\n",
    "\n",
    "par = {\"subreddit\": \"climatechange\", #The subreddit title\n",
    "       \"post_num\": 100, # Numer of posts to pull from\n",
    "        \"time_1\": int(time.mktime(time.strptime('1 July, 2020', '%d %B, %Y'))), # The latest pull time\n",
    "       \"API_limit\": 100, # API pull number limits for reddit per time\n",
    "       \"API_wait\": 1 #API wait time berfore the next pull\n",
    "      }\n",
    "\n",
    "\n",
    "\n",
    "df_test_reddit = get_reddit_posts(par[\"subreddit\"], par[\"post_num\"], par[\"time_1\"], par[ \"API_limit\"], par[\"API_wait\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>domain</th>\n",
       "      <th>id</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>over_18</th>\n",
       "      <th>post_hint</th>\n",
       "      <th>score</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Newman1651</td>\n",
       "      <td>1593569625</td>\n",
       "      <td>en.mercopress.com</td>\n",
       "      <td>hj1f11</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>25629</td>\n",
       "      <td>Sea ice in the Weddell Sea has decreased by on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CharlieBrown829</td>\n",
       "      <td>1593569315</td>\n",
       "      <td>self.climatechange</td>\n",
       "      <td>hj1c7i</td>\n",
       "      <td>60</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>25629</td>\n",
       "      <td>Are greenhouse gas emissions still going down ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LackmustestTester</td>\n",
       "      <td>1593555082</td>\n",
       "      <td>nature.com</td>\n",
       "      <td>hixhvn</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>link</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>25621</td>\n",
       "      <td>Floodplain inundation spectrum across the Unit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>iamchitranjanbaghi</td>\n",
       "      <td>1593550152</td>\n",
       "      <td>self.climatechange</td>\n",
       "      <td>hivz86</td>\n",
       "      <td>20</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>co2 becomes liquid under pressure and mariana ...</td>\n",
       "      <td>25619</td>\n",
       "      <td>putting co2 in deep mariana trench.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>coffeecream22</td>\n",
       "      <td>1593549986</td>\n",
       "      <td>self.climatechange</td>\n",
       "      <td>hivxd7</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>self</td>\n",
       "      <td>1</td>\n",
       "      <td>He [posted a long letter](https://environmenta...</td>\n",
       "      <td>25619</td>\n",
       "      <td>Does anyone have science-based responses to Mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Gorgulak</td>\n",
       "      <td>1593542301</td>\n",
       "      <td>nature.com</td>\n",
       "      <td>hitako</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>link</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>25617</td>\n",
       "      <td>New multi-method ensemble approach to reconstr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ChargersPalkia</td>\n",
       "      <td>1593537578</td>\n",
       "      <td>self.climatechange</td>\n",
       "      <td>hirohk</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>self</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.npr.org/2019/02/05/691734652/the-n...</td>\n",
       "      <td>25612</td>\n",
       "      <td>Is this legit? Someone sent me this link about...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TheMineInventer</td>\n",
       "      <td>1593533686</td>\n",
       "      <td>self.climatechange</td>\n",
       "      <td>hiqdy2</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>25610</td>\n",
       "      <td>The main mod of this server is also the mod of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AndeanBear101</td>\n",
       "      <td>1593531549</td>\n",
       "      <td>self.climatechange</td>\n",
       "      <td>hippaf</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>[removed]\\n\\n[View Poll](https://www.reddit.co...</td>\n",
       "      <td>25609</td>\n",
       "      <td>Climate footprint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AndeanBear101</td>\n",
       "      <td>1593531504</td>\n",
       "      <td>self.climatechange</td>\n",
       "      <td>hipoqc</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>25610</td>\n",
       "      <td>Cutting carbon footprint in home</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author  created_utc              domain      id  num_comments  \\\n",
       "0          Newman1651   1593569625   en.mercopress.com  hj1f11             1   \n",
       "1     CharlieBrown829   1593569315  self.climatechange  hj1c7i            60   \n",
       "2   LackmustestTester   1593555082          nature.com  hixhvn             2   \n",
       "3  iamchitranjanbaghi   1593550152  self.climatechange  hivz86            20   \n",
       "4       coffeecream22   1593549986  self.climatechange  hivxd7            32   \n",
       "5            Gorgulak   1593542301          nature.com  hitako             1   \n",
       "6      ChargersPalkia   1593537578  self.climatechange  hirohk             5   \n",
       "7     TheMineInventer   1593533686  self.climatechange  hiqdy2             0   \n",
       "8       AndeanBear101   1593531549  self.climatechange  hippaf             0   \n",
       "9       AndeanBear101   1593531504  self.climatechange  hipoqc             0   \n",
       "\n",
       "   over_18 post_hint  score  \\\n",
       "0    False       NaN      1   \n",
       "1    False       NaN      1   \n",
       "2    False      link      1   \n",
       "3    False       NaN      1   \n",
       "4    False      self      1   \n",
       "5    False      link      2   \n",
       "6    False      self      1   \n",
       "7    False       NaN      3   \n",
       "8    False       NaN      1   \n",
       "9    False       NaN      1   \n",
       "\n",
       "                                            selftext  subreddit_subscribers  \\\n",
       "0                                                                     25629   \n",
       "1                                                                     25629   \n",
       "2                                                                     25621   \n",
       "3  co2 becomes liquid under pressure and mariana ...                  25619   \n",
       "4  He [posted a long letter](https://environmenta...                  25619   \n",
       "5                                                                     25617   \n",
       "6  https://www.npr.org/2019/02/05/691734652/the-n...                  25612   \n",
       "7                                          [removed]                  25610   \n",
       "8  [removed]\\n\\n[View Poll](https://www.reddit.co...                  25609   \n",
       "9                                          [removed]                  25610   \n",
       "\n",
       "                                               title  \n",
       "0  Sea ice in the Weddell Sea has decreased by on...  \n",
       "1  Are greenhouse gas emissions still going down ...  \n",
       "2  Floodplain inundation spectrum across the Unit...  \n",
       "3                putting co2 in deep mariana trench.  \n",
       "4  Does anyone have science-based responses to Mi...  \n",
       "5  New multi-method ensemble approach to reconstr...  \n",
       "6  Is this legit? Someone sent me this link about...  \n",
       "7  The main mod of this server is also the mod of...  \n",
       "8                                  Climate footprint  \n",
       "9                   Cutting carbon footprint in home  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_keep_clmns = ['author', 'created_utc', 'domain', 'id', 'num_comments', 'over_18',\n",
    "       'post_hint', 'score', 'selftext',\n",
    "       'subreddit_subscribers', 'title']\n",
    "\n",
    "df_test_reddit = df_test_reddit[to_keep_clmns]\n",
    "\n",
    "df_keep_reddit = df_test_reddit.copy()\n",
    "\n",
    "df_test_reddit.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For title and selftext columns, I filled them with \" \" as they will be striped later, so I can merge them later.\n",
    "\n",
    "df_test_reddit[\"title\"].fillna(\" \", inplace=True)\n",
    "df_test_reddit[\"selftext\"].fillna(\" \", inplace=True)\n",
    "\n",
    "#Merging the title and selftext for further processing\n",
    "\n",
    "df_test_reddit['text_merged'] = df_test_reddit['title'] + \" \" + df_test_reddit['selftext']\n",
    "df_test_reddit.drop(columns = [\"title\", \"selftext\"], inplace=True)\n",
    "\n",
    "#For subscribers, I imputed them with median valeus\n",
    "df_test_reddit['subreddit_subscribers'].fillna(df_test_reddit['subreddit_subscribers'].median(), inplace=True)\n",
    "\n",
    "#For post_hint, I imputed them with \"Empty\"\n",
    "df_test_reddit['post_hint'].fillna(\"Empty\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lots pof cleaning on text\n",
    "\n",
    "\n",
    "#Removing \"\\n\" characters\n",
    "df_test_reddit['text_merged'] = df_test_reddit['text_merged'].apply(lambda x: re.sub('\\n', ' ', x))\n",
    "\n",
    "#Removing the [removed] characters\n",
    "df_test_reddit['text_merged'] = df_test_reddit['text_merged'].replace('[removed]', ' ')\n",
    "\n",
    "\n",
    "# Use regular expressions to do a find-and-replace\n",
    "df_test_reddit['text_merged'] = df_test_reddit['text_merged'].apply(lambda x: re.sub(\"[^a-zA-Z]\", \" \", x))\n",
    "\n",
    "df_test_reddit['text_merged'] = df_test_reddit['text_merged'].apply(lambda x: x.lower())\n",
    "\n",
    "\n",
    "\n",
    "#Laste step\n",
    "#Replacing multiple spaces\n",
    "#Source: https://pythonexamples.org/python-replace-multiple-spaces-with-single-space-in-text-file/\n",
    "df_test_reddit['text_merged'] = df_test_reddit['text_merged'].apply(lambda x: ' '.join(x.split()))\n",
    "\n",
    "\n",
    "#Removing stop words and stemming\n",
    "\n",
    "def remove_stops_stem(item):\n",
    "\n",
    "    stops = stopwords.words('english')\n",
    "    words = [w for w in item.split() if w not in stops]#stops\n",
    "    \n",
    "    #lemmatizer = WordNetLemmatizer()\n",
    "    #words = [lemmatizer.lemmatize(i) for i in words]\n",
    "    \n",
    "    # Instantiate object of class PorterStemmer.\n",
    "    p_stemmer = PorterStemmer()\n",
    "    words = [p_stemmer.stem(i) for i in words]\n",
    "    \n",
    "    \n",
    "    \n",
    "    words = \" \".join(list(words)) # Adding space\n",
    "    \n",
    "    return words\n",
    "\n",
    "df_test_reddit['text_merged'] = df_test_reddit['text_merged'].apply(remove_stops_stem)\n",
    "\n",
    "\n",
    "\n",
    "#Stemming\n",
    "\n",
    "\n",
    "df_test_reddit.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineering the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting the charcaters and word in \"text_merged\"\n",
    "df_test_reddit[\"text_char_count\"] = df_test_reddit[\"text_merged\"].map(lambda x: len(x))\n",
    "df_test_reddit[\"text_word_count\"] = df_test_reddit[\"text_merged\"].map(lambda x: len(x.split(\" \")))\n",
    "\n",
    "#Sentiment analyzer\n",
    "sent = SentimentIntensityAnalyzer()\n",
    "\n",
    "df_test_reddit['sentiment_score'] = df_test_reddit[\"text_merged\"].apply(lambda x: sent.polarity_scores(x)['compound'])\n",
    "\n",
    "\n",
    "df_test_reddit['date'] = pd.to_datetime(df_test_reddit['created_utc'],unit='s')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uwa(item):\n",
    "\n",
    "    words = [w for w in item.split()]\n",
    "    #print(words)\n",
    "    \n",
    "    index_list = []\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        \n",
    "        if (words[i] in list(top_words)):\n",
    "            index_list.append(i)\n",
    "    \n",
    "    dist = 0\n",
    "    \n",
    "    dist = np.sum(np.array(index_list[1:]) - np.array(index_list[0:-1]))\n",
    "    \n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = pickle.load(open('../datasets/top_words_overlap.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_reddit['wua'] = df_test_reddit['text_merged'].apply(get_uwa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the pickled trained model and testing it on the new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = pickle.load(open('../datasets/lr.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_test_reddit['text_merged']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = pickle.load(open('../datasets/vectorizer.pkl', 'rb'))\n",
    "list_stop_words = [\"dec\", \"global\", \"http\", \"www\", \"com\", \"conspiraci\", \"warm\", \"climat\", \"remov\", \"theori\", \"theactualshadow\", \"co\"]\n",
    "\n",
    "\n",
    "\n",
    "# vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "#                              #ngram_range=(1,2),\n",
    "#                              tokenizer = None,\n",
    "#                              preprocessor = None,\n",
    "#                              stop_words = list_stop_words,\n",
    "#                              max_features = 3000) \n",
    "\n",
    "\n",
    "#X = vectorizer.fit_transform(X_train)\n",
    "\n",
    "#pickle.dump(vectorizer, open('../datasets/vectorizer.pkl', 'wb'))\n",
    "\n",
    "X_features = vectorizer.transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr.predict(X_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>domain</th>\n",
       "      <th>id</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>over_18</th>\n",
       "      <th>post_hint</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>text_merged</th>\n",
       "      <th>text_char_count</th>\n",
       "      <th>text_word_count</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>date</th>\n",
       "      <th>wua</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Newman1651</td>\n",
       "      <td>1593569625</td>\n",
       "      <td>en.mercopress.com</td>\n",
       "      <td>hj1f11</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Empty</td>\n",
       "      <td>1</td>\n",
       "      <td>25629</td>\n",
       "      <td>sea ice weddel sea decreas one million sq kilo...</td>\n",
       "      <td>59</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2020-07-01 02:13:45</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CharlieBrown829</td>\n",
       "      <td>1593569315</td>\n",
       "      <td>self.climatechange</td>\n",
       "      <td>hj1c7i</td>\n",
       "      <td>60</td>\n",
       "      <td>False</td>\n",
       "      <td>Empty</td>\n",
       "      <td>1</td>\n",
       "      <td>25629</td>\n",
       "      <td>greenhous ga emiss still go global due pandem</td>\n",
       "      <td>45</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2020-07-01 02:08:35</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LackmustestTester</td>\n",
       "      <td>1593555082</td>\n",
       "      <td>nature.com</td>\n",
       "      <td>hixhvn</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>link</td>\n",
       "      <td>1</td>\n",
       "      <td>25621</td>\n",
       "      <td>floodplain inund spectrum across unit state</td>\n",
       "      <td>43</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2020-06-30 22:11:22</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>iamchitranjanbaghi</td>\n",
       "      <td>1593550152</td>\n",
       "      <td>self.climatechange</td>\n",
       "      <td>hivz86</td>\n",
       "      <td>20</td>\n",
       "      <td>False</td>\n",
       "      <td>Empty</td>\n",
       "      <td>1</td>\n",
       "      <td>25619</td>\n",
       "      <td>put co deep mariana trench co becom liquid pre...</td>\n",
       "      <td>234</td>\n",
       "      <td>41</td>\n",
       "      <td>-0.7506</td>\n",
       "      <td>2020-06-30 20:49:12</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>coffeecream22</td>\n",
       "      <td>1593549986</td>\n",
       "      <td>self.climatechange</td>\n",
       "      <td>hivxd7</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>self</td>\n",
       "      <td>1</td>\n",
       "      <td>25619</td>\n",
       "      <td>anyon scienc base respons michael shellenberg ...</td>\n",
       "      <td>259</td>\n",
       "      <td>36</td>\n",
       "      <td>-0.7430</td>\n",
       "      <td>2020-06-30 20:46:26</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author  created_utc              domain      id  num_comments  \\\n",
       "0          Newman1651   1593569625   en.mercopress.com  hj1f11             1   \n",
       "1     CharlieBrown829   1593569315  self.climatechange  hj1c7i            60   \n",
       "2   LackmustestTester   1593555082          nature.com  hixhvn             2   \n",
       "3  iamchitranjanbaghi   1593550152  self.climatechange  hivz86            20   \n",
       "4       coffeecream22   1593549986  self.climatechange  hivxd7            32   \n",
       "\n",
       "   over_18 post_hint  score  subreddit_subscribers  \\\n",
       "0    False     Empty      1                  25629   \n",
       "1    False     Empty      1                  25629   \n",
       "2    False      link      1                  25621   \n",
       "3    False     Empty      1                  25619   \n",
       "4    False      self      1                  25619   \n",
       "\n",
       "                                         text_merged  text_char_count  \\\n",
       "0  sea ice weddel sea decreas one million sq kilo...               59   \n",
       "1      greenhous ga emiss still go global due pandem               45   \n",
       "2        floodplain inund spectrum across unit state               43   \n",
       "3  put co deep mariana trench co becom liquid pre...              234   \n",
       "4  anyon scienc base respons michael shellenberg ...              259   \n",
       "\n",
       "   text_word_count  sentiment_score                date   wua  \n",
       "0               11           0.0000 2020-07-01 02:13:45   5.0  \n",
       "1                8           0.0000 2020-07-01 02:08:35   0.0  \n",
       "2                6           0.0000 2020-06-30 22:11:22   0.0  \n",
       "3               41          -0.7506 2020-06-30 20:49:12  10.0  \n",
       "4               36          -0.7430 2020-06-30 20:46:26  20.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_reddit.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_reddit[\"preds\"] = pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Filtered posts as non-global warming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([7, 39, 43, 47, 55, 91], dtype='int64')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = df_test_reddit[df_test_reddit[\"preds\"]==0].index\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7         main mod server also mod r climateskept remov\n",
       "39    facebook creat fact check exempt climat denier...\n",
       "43    jeff bezo say amazon bought name right climat ...\n",
       "47    lesson pandem hi would like know lesson take c...\n",
       "55    look volunt design skill part non profit proje...\n",
       "91    climat movement must unit behind black live ma...\n",
       "Name: text_merged, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_reddit.loc[indices,:][\"text_merged\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sea ice weddel sea decreas one million sq kilomet five year'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_reddit.loc[0,:][\"text_merged\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lessons from the pandemic'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_keep_reddit.loc[47,:][\"title\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Filtered posts as global warming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([ 0,  1,  2,  3,  4,  5,  6,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "            18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "            35, 36, 37, 38, 40, 41, 42, 44, 45, 46, 48, 49, 50, 51, 52, 53, 54,\n",
       "            56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,\n",
       "            73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n",
       "            90, 92, 93, 94, 95, 96, 97, 98, 99],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = df_test_reddit[df_test_reddit[\"preds\"]==1].index\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     sea ice weddel sea decreas one million sq kilo...\n",
       "1         greenhous ga emiss still go global due pandem\n",
       "2           floodplain inund spectrum across unit state\n",
       "3     put co deep mariana trench co becom liquid pre...\n",
       "4     anyon scienc base respons michael shellenberg ...\n",
       "                            ...                        \n",
       "95                     ghg emiss realli reduc due covid\n",
       "96    eco friendli altern cement might save world su...\n",
       "97                power work fossil fuel industri remov\n",
       "98                             extinct rebellion realli\n",
       "99                         climat sceptic mod sub remov\n",
       "Name: text_merged, Length: 94, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_reddit.loc[indices,:][\"text_merged\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'power work fossil fuel industri remov'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_reddit.loc[97,:][\"text_merged\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World !\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello World !\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
