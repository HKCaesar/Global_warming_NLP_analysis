{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The global warming issue and Narratives around it<br>\n",
    "### Part 6: Testing the trained model onto a new datasaet\n",
    "\n",
    "## Importing new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "#Importing the built function, prior to that added the assets path to the system path\n",
    "#Inspiration: https://stackoverflow.com/questions/4383571/importing-files-from-different-folder\n",
    "\n",
    "import sys\n",
    "# inserting the parent directory into current path\n",
    "sys.path.insert(1, '../assets')\n",
    "\n",
    "from get_reddit_posts import get_reddit_posts\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from nltk.corpus import stopwords # Import the stopword list\n",
    "import pickle\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import regex as re\n",
    "import nltk\n",
    "#nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 posts downloaded, oldest post:2020-06-20 10:40:38 - status code: 200, now waiting 1 seconds before next pull. Patience...\n",
      "\u001b[92m Does the imported dataframe match the request? True\u001b[00m\n",
      "Final DataFrame shape: (100, 71), there are 0 duplicates\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Defining API pull initial parameteres:\n",
    "\n",
    "par = {\"subreddit\": \"climatechange\", #The subreddit title\n",
    "       \"post_num\": 100, # Numer of posts to pull from\n",
    "        \"time_1\": int(time.mktime(time.strptime('1 July, 2020', '%d %B, %Y'))), # The latest pull time\n",
    "       \"API_limit\": 100, # API pull number limits for reddit per time\n",
    "       \"API_wait\": 1 #API wait time berfore the next pull\n",
    "      }\n",
    "\n",
    "\n",
    "\n",
    "df_test_reddit = get_reddit_posts(par[\"subreddit\"], par[\"post_num\"], par[\"time_1\"], par[ \"API_limit\"], par[\"API_wait\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cleaning the new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>domain</th>\n",
       "      <th>id</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>over_18</th>\n",
       "      <th>post_hint</th>\n",
       "      <th>score</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Newman1651</td>\n",
       "      <td>1593569625</td>\n",
       "      <td>en.mercopress.com</td>\n",
       "      <td>hj1f11</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>25629</td>\n",
       "      <td>Sea ice in the Weddell Sea has decreased by on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CharlieBrown829</td>\n",
       "      <td>1593569315</td>\n",
       "      <td>self.climatechange</td>\n",
       "      <td>hj1c7i</td>\n",
       "      <td>60</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>25629</td>\n",
       "      <td>Are greenhouse gas emissions still going down ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LackmustestTester</td>\n",
       "      <td>1593555082</td>\n",
       "      <td>nature.com</td>\n",
       "      <td>hixhvn</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>link</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>25621</td>\n",
       "      <td>Floodplain inundation spectrum across the Unit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>iamchitranjanbaghi</td>\n",
       "      <td>1593550152</td>\n",
       "      <td>self.climatechange</td>\n",
       "      <td>hivz86</td>\n",
       "      <td>20</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>co2 becomes liquid under pressure and mariana ...</td>\n",
       "      <td>25619</td>\n",
       "      <td>putting co2 in deep mariana trench.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>coffeecream22</td>\n",
       "      <td>1593549986</td>\n",
       "      <td>self.climatechange</td>\n",
       "      <td>hivxd7</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>self</td>\n",
       "      <td>1</td>\n",
       "      <td>He [posted a long letter](https://environmenta...</td>\n",
       "      <td>25619</td>\n",
       "      <td>Does anyone have science-based responses to Mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Gorgulak</td>\n",
       "      <td>1593542301</td>\n",
       "      <td>nature.com</td>\n",
       "      <td>hitako</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>link</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>25617</td>\n",
       "      <td>New multi-method ensemble approach to reconstr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ChargersPalkia</td>\n",
       "      <td>1593537578</td>\n",
       "      <td>self.climatechange</td>\n",
       "      <td>hirohk</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>self</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.npr.org/2019/02/05/691734652/the-n...</td>\n",
       "      <td>25612</td>\n",
       "      <td>Is this legit? Someone sent me this link about...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TheMineInventer</td>\n",
       "      <td>1593533686</td>\n",
       "      <td>self.climatechange</td>\n",
       "      <td>hiqdy2</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>25610</td>\n",
       "      <td>The main mod of this server is also the mod of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AndeanBear101</td>\n",
       "      <td>1593531549</td>\n",
       "      <td>self.climatechange</td>\n",
       "      <td>hippaf</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>[removed]\\n\\n[View Poll](https://www.reddit.co...</td>\n",
       "      <td>25609</td>\n",
       "      <td>Climate footprint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AndeanBear101</td>\n",
       "      <td>1593531504</td>\n",
       "      <td>self.climatechange</td>\n",
       "      <td>hipoqc</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>25610</td>\n",
       "      <td>Cutting carbon footprint in home</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author  created_utc              domain      id  num_comments  \\\n",
       "0          Newman1651   1593569625   en.mercopress.com  hj1f11             1   \n",
       "1     CharlieBrown829   1593569315  self.climatechange  hj1c7i            60   \n",
       "2   LackmustestTester   1593555082          nature.com  hixhvn             2   \n",
       "3  iamchitranjanbaghi   1593550152  self.climatechange  hivz86            20   \n",
       "4       coffeecream22   1593549986  self.climatechange  hivxd7            32   \n",
       "5            Gorgulak   1593542301          nature.com  hitako             1   \n",
       "6      ChargersPalkia   1593537578  self.climatechange  hirohk             5   \n",
       "7     TheMineInventer   1593533686  self.climatechange  hiqdy2             0   \n",
       "8       AndeanBear101   1593531549  self.climatechange  hippaf             0   \n",
       "9       AndeanBear101   1593531504  self.climatechange  hipoqc             0   \n",
       "\n",
       "   over_18 post_hint  score  \\\n",
       "0    False       NaN      1   \n",
       "1    False       NaN      1   \n",
       "2    False      link      1   \n",
       "3    False       NaN      1   \n",
       "4    False      self      1   \n",
       "5    False      link      2   \n",
       "6    False      self      1   \n",
       "7    False       NaN      3   \n",
       "8    False       NaN      1   \n",
       "9    False       NaN      1   \n",
       "\n",
       "                                            selftext  subreddit_subscribers  \\\n",
       "0                                                                     25629   \n",
       "1                                                                     25629   \n",
       "2                                                                     25621   \n",
       "3  co2 becomes liquid under pressure and mariana ...                  25619   \n",
       "4  He [posted a long letter](https://environmenta...                  25619   \n",
       "5                                                                     25617   \n",
       "6  https://www.npr.org/2019/02/05/691734652/the-n...                  25612   \n",
       "7                                          [removed]                  25610   \n",
       "8  [removed]\\n\\n[View Poll](https://www.reddit.co...                  25609   \n",
       "9                                          [removed]                  25610   \n",
       "\n",
       "                                               title  \n",
       "0  Sea ice in the Weddell Sea has decreased by on...  \n",
       "1  Are greenhouse gas emissions still going down ...  \n",
       "2  Floodplain inundation spectrum across the Unit...  \n",
       "3                putting co2 in deep mariana trench.  \n",
       "4  Does anyone have science-based responses to Mi...  \n",
       "5  New multi-method ensemble approach to reconstr...  \n",
       "6  Is this legit? Someone sent me this link about...  \n",
       "7  The main mod of this server is also the mod of...  \n",
       "8                                  Climate footprint  \n",
       "9                   Cutting carbon footprint in home  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_keep_clmns = ['author', 'created_utc', 'domain', 'id', 'num_comments', 'over_18',\n",
    "       'post_hint', 'score', 'selftext',\n",
    "       'subreddit_subscribers', 'title']\n",
    "\n",
    "df_test_reddit = df_test_reddit[to_keep_clmns]\n",
    "\n",
    "df_keep_reddit = df_test_reddit.copy()\n",
    "\n",
    "df_test_reddit.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For title and selftext columns, I filled them with \" \" as they will be striped later, so I can merge them later.\n",
    "\n",
    "df_test_reddit[\"title\"].fillna(\" \", inplace=True)\n",
    "df_test_reddit[\"selftext\"].fillna(\" \", inplace=True)\n",
    "\n",
    "#Merging the title and selftext for further processing\n",
    "\n",
    "df_test_reddit['text_merged'] = df_test_reddit['title'] + \" \" + df_test_reddit['selftext']\n",
    "df_test_reddit.drop(columns = [\"title\", \"selftext\"], inplace=True)\n",
    "\n",
    "#For subscribers, I imputed them with median valeus\n",
    "df_test_reddit['subreddit_subscribers'].fillna(df_test_reddit['subreddit_subscribers'].median(), inplace=True)\n",
    "\n",
    "#For post_hint, I imputed them with \"Empty\"\n",
    "df_test_reddit['post_hint'].fillna(\"Empty\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lots pof cleaning on text\n",
    "\n",
    "\n",
    "#Removing \"\\n\" characters\n",
    "df_test_reddit['text_merged'] = df_test_reddit['text_merged'].apply(lambda x: re.sub('\\n', ' ', x))\n",
    "\n",
    "#Removing the [removed] characters\n",
    "df_test_reddit['text_merged'] = df_test_reddit['text_merged'].replace('[removed]', ' ')\n",
    "\n",
    "\n",
    "# Use regular expressions to do a find-and-replace\n",
    "df_test_reddit['text_merged'] = df_test_reddit['text_merged'].apply(lambda x: re.sub(\"[^a-zA-Z]\", \" \", x))\n",
    "\n",
    "df_test_reddit['text_merged'] = df_test_reddit['text_merged'].apply(lambda x: x.lower())\n",
    "\n",
    "\n",
    "\n",
    "#Laste step\n",
    "#Replacing multiple spaces\n",
    "#Source: https://pythonexamples.org/python-replace-multiple-spaces-with-single-space-in-text-file/\n",
    "df_test_reddit['text_merged'] = df_test_reddit['text_merged'].apply(lambda x: ' '.join(x.split()))\n",
    "\n",
    "\n",
    "#Removing stop words and stemming\n",
    "\n",
    "def remove_stops_stem(item):\n",
    "\n",
    "    stops = stopwords.words('english')\n",
    "    words = [w for w in item.split() if w not in stops]#stops\n",
    "    \n",
    "    #lemmatizer = WordNetLemmatizer()\n",
    "    #words = [lemmatizer.lemmatize(i) for i in words]\n",
    "    \n",
    "    # Instantiate object of class PorterStemmer.\n",
    "    p_stemmer = PorterStemmer()\n",
    "    words = [p_stemmer.stem(i) for i in words]\n",
    "    \n",
    "    \n",
    "    \n",
    "    words = \" \".join(list(words)) # Adding space\n",
    "    \n",
    "    return words\n",
    "\n",
    "df_test_reddit['text_merged'] = df_test_reddit['text_merged'].apply(remove_stops_stem)\n",
    "\n",
    "\n",
    "\n",
    "#Stemming\n",
    "\n",
    "\n",
    "df_test_reddit.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting the charcaters and word in \"text_merged\"\n",
    "df_test_reddit[\"text_char_count\"] = df_test_reddit[\"text_merged\"].map(lambda x: len(x))\n",
    "df_test_reddit[\"text_word_count\"] = df_test_reddit[\"text_merged\"].map(lambda x: len(x.split(\" \")))\n",
    "\n",
    "#Sentiment analyzer\n",
    "sent = SentimentIntensityAnalyzer()\n",
    "\n",
    "df_test_reddit['sentiment_score'] = df_test_reddit[\"text_merged\"].apply(lambda x: sent.polarity_scores(x)['compound'])\n",
    "\n",
    "\n",
    "df_test_reddit['date'] = pd.to_datetime(df_test_reddit['created_utc'],unit='s')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uwa(item):\n",
    "\n",
    "    words = [w for w in item.split()]\n",
    "    #print(words)\n",
    "    \n",
    "    index_list = []\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        \n",
    "        if (words[i] in list(top_words)):\n",
    "            index_list.append(i)\n",
    "    \n",
    "    dist = 0\n",
    "    \n",
    "    dist = np.sum(np.array(index_list[1:]) - np.array(index_list[0:-1]))\n",
    "    \n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../datasets/top_words_overlap.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a6a9a94b95b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../datasets/top_words_overlap.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../datasets/top_words_overlap.pkl'"
     ]
    }
   ],
   "source": [
    "top_words = pickle.load(open('../datasets/top_words_overlap.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_reddit['wua'] = df_test_reddit['text_merged'].apply(get_uwa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = pickle.load(open('../datasets/lr.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_test_reddit['text_merged']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = pickle.load(open('../datasets/vectorizer.pkl', 'rb'))\n",
    "list_stop_words = [\"dec\", \"global\", \"http\", \"www\", \"com\", \"conspiraci\", \"warm\", \"climat\", \"remov\", \"theori\", \"theactualshadow\", \"co\"]\n",
    "\n",
    "\n",
    "\n",
    "# vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "#                              #ngram_range=(1,2),\n",
    "#                              tokenizer = None,\n",
    "#                              preprocessor = None,\n",
    "#                              stop_words = list_stop_words,\n",
    "#                              max_features = 3000) \n",
    "\n",
    "\n",
    "#X = vectorizer.fit_transform(X_train)\n",
    "\n",
    "#pickle.dump(vectorizer, open('../datasets/vectorizer.pkl', 'wb'))\n",
    "\n",
    "X_features = vectorizer.transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr.predict(X_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pred.sum())/len(df_test_reddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_reddit.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_reddit[\"preds\"] = pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = df_test_reddit[df_test_reddit[\"preds\"]==0].index\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_reddit.loc[indices,:][\"text_merged\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_reddit.loc[0,:][\"text_merged\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keep_reddit.loc[47,:][\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = df_test_reddit[df_test_reddit[\"preds\"]==1].index\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_reddit.loc[indices,:][\"text_merged\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_reddit.loc[97,:][\"text_merged\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
